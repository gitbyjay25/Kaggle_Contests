# -*- coding: utf-8 -*-
"""notebook34bb5c8cd8

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/gitbyjay25/notebook34bb5c8cd8.8c559408-06dc-43b4-b8b1-23aa624658c3.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251025/auto/storage/goog4_request%26X-Goog-Date%3D20251025T030437Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da7a21cd27a4af33a877876c26899c00b08595449abe4f30025600fd11c978d56a61f65f8a94418789a36a4a6a0051f0435e2350fc8f27f7469648e6b74aef640d11c657edbead8500741bd12af3f1c4ac1c48f7121582c187f6d79ae366f7f2bbd6e1dcbe971741fe01de03cb6e5b449a4ed7fd1412613345cfbfbb790c6ce878b72936c95a526d09200442e13237c3b12527c984acc1dfb172a43d4e4528a4525c1409b6ead6b729dc0731f6b81756c1c6ae35ed622749b3df71fa52c54c20da4a40b65591833954394094192f01bff4b710a99508a44148703592f58d473e7b4baf4a12402f0c778c161983cdf7833221e02dfc4c0a1e11c7c7862621d9d1f
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('train.csv')
df.head()

sns.heatmap(df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1).corr(), cmap="YlGnBu")
plt.show()

from sklearn.model_selection  import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2)
for train_indices , test_indices in split.split(df,df[["Survived","Pclass","Sex"]]):
    strat_train_set=df.loc[train_indices]
    strat_test_set=df.loc[test_indices]

strat_test_set

plt.subplot(1,2,1)
strat_train_set['Survived'].hist()
strat_train_set['Pclass'].hist()

plt.subplot(1,2,2)
strat_test_set['Survived'].hist()
strat_test_set['Pclass'].hist()
plt.show()

#find misiing value

strat_train_set.info()

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer
class AgeImputer(BaseEstimator, TransformerMixin):
    def fit(self,X,y=None):
        return self
    def transform(self,X):
        imputer=SimpleImputer(strategy='median')
        X['Age']=imputer.fit_transform(X[['Age']])
        return X

#encoding
from sklearn.preprocessing import OneHotEncoder

class FeatureEncoder(BaseEstimator,TransformerMixin):
    def fit(self,X,y=None):
        return self
    def transform(self,X):
        encoder=OneHotEncoder()
        matrix=encoder.fit_transform(X[['Embarked']]).toarray()

        column_names = ["C","S","Q","N"]

        for i in range(len(matrix.T)):
            X[column_names[i]]=matrix.T[i]

        matrix=encoder.fit_transform(X[['Sex']]).toarray()

        column_names = ["Male","Female"]

        for i in range(len(matrix.T)):
            X[column_names[i]]=matrix.T[i]
        return X

class FeatureDropper(BaseEstimator,TransformerMixin):
    def fit(self,X,y=None):
        return self
    def transform(self,X):
        return X.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked',"N"], axis=1,errors="ignore")

from sklearn.pipeline import Pipeline
pipeline=Pipeline([('age_imputer',AgeImputer()),
               ('feature_encoder',FeatureEncoder()),
    ('feature_dropper',FeatureDropper())
])

strat_train_set = pipeline.fit_transform(strat_train_set)

strat_train_set.info()

from sklearn.preprocessing import StandardScaler

X=strat_train_set.drop('Survived',axis=1)
y=strat_train_set['Survived']


scaler=StandardScaler()
X_data=scaler.fit_transform(X)
Y_data=y.to_numpy()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

cif = RandomForestClassifier()

param_grid =[
{"n_estimators":[10,100,200,500],"max_depth": [None,5,10],"min_samples_split":[2,3,4]}
]

grid_search=GridSearchCV(cif,param_grid,cv=3,scoring="accuracy",return_train_score=True)
grid_search.fit(X_data,Y_data)

final_clf =grid_search.best_estimator_

final_clf

strat_test_set = pipeline.fit_transform(strat_test_set)

X_test = strat_test_set.drop(["Survived"],axis=1)
y_test = strat_test_set["Survived"]

scaler = StandardScaler()
X_data_test = scaler.fit_transform(X_test)
y_data_test = y_test.to_numpy()

final_clf.score(X_data_test,y_data_test)

final_data=pipeline.fit_transform(df)

final_data

X_final = final_data.drop(["Survived"],axis=1)
y_final = final_data["Survived"]

scaler = StandardScaler()
X_data_final = scaler.fit_transform(X_final)

prod_clf = RandomForestClassifier()

param_grid = [
    {"n_estimators":[10,100,200,500],"max_depth": [None,5,10],"min_samples_split":[2,3,4]}
]

grid_search = GridSearchCV(prod_clf,param_grid,cv=3,scoring="accuracy",return_train_score=True)
grid_search.fit(X_data_final,y_final)

prod_final_clf =grid_search.best_estimator_

prod_final_clf

titanic_test_data=pd.read_csv('test.csv')

final_test_data = pipeline.fit_transform(titanic_test_data)

X_final_test = final_test_data
X_data_final_test = X_final_test.fillna(method="ffill")

scaler = StandardScaler()
X_data_final_test = scaler.fit_transform(X_final_test)

predictions=prod_final_clf.predict(X_data_final_test)

final_df=pd.DataFrame(titanic_test_data['PassengerId'])
final_df['Survived']=predictions
final_df.to_csv('Titanic_pred.csv',index=False)

final_df

